{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libs\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# Mongo DB\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from MongoDB\n",
    "DBClient = MongoClient()\n",
    "yelp_data = DBClient.yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select business having atleast 50 reviews\n",
    "min_review_count = 50\n",
    "\n",
    "# businesses to Analyse\n",
    "businesses_to_analyse = 'Restaurants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all restaurant businesses\n",
    "Restaurant_business = pd.DataFrame(yelp_data.business.find({\"categories\":{\"$regex\" :\".*\"+businesses_to_analyse+\".*\"}, \"review_count\":{\"$gte\":min_review_count} },  {'business_id':1, 'name':1, 'city':1, 'state':1, 'stars':1, 'review_count':1, 'categories':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all reviews\n",
    "All_reviews = pd.DataFrame(yelp_data.review.find({},{'review_id':1, 'user_id':1, 'business_id':1, 'stars':1, 'useful':1, 'text':1, 'date':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all restaurant reviews\n",
    "#Restaurant_reviews = All_reviews[All_reviews.business_id.isin(Restaurant_business.business_id.values)]\n",
    "Restaurant_reviews = pd.merge(Restaurant_business,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 Restaurant\n",
    "Restaurant_business.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 Reviews\n",
    "Restaurant_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected Restaurants to file\n",
    "Restaurant_reviews.to_csv('processed_data/restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected Restaurant-reviews to file\n",
    "Restaurant_business.to_csv('processed_data/restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many reviews we have of each star\n",
    "star_x = Restaurant_reviews.review_stars.value_counts().index\n",
    "star_y = Restaurant_reviews.review_stars.value_counts().values\n",
    "\n",
    "plot.figure(figsize=(8,5))\n",
    "# colors are in the order 5, 4, 3, 1, 2\n",
    "bar_colors = ['darkgreen', 'mediumseagreen', 'gold', 'crimson', 'orange']\n",
    "plot.bar(star_x, star_y, color=bar_colors, width=.6)\n",
    "plot.xlabel('Stars (Rating)')\n",
    "plot.ylabel('Number of Reviews')\n",
    "plot.title(f'Number of Reviews Per Rating of {businesses_to_analyse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_business.groupby('state').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_per_state = Restaurant_business.groupby('state').count()[['business_id']].rename(columns={'state': 'State', 'business_id': 'Restaurants'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_per_state.sort_values(by='Restaurants').plot.bar(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ = pd.DataFrame(yelp_data.business.find({\"categories\":{\"$regex\" :\".*\"+businesses_to_analyse+\".*\"}, \"review_count\":{\"$gte\":min_review_count}, \"state\":\"AZ\" },  {'business_id':1, 'name':1, 'city':1, 'state':1, 'stars':1, 'review_count':1, 'categories':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews = pd.merge(Restaurant_AZ,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews.to_csv('processed_data/restaurant_az_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ.to_csv('processed_data/restaurants_az.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many reviews we have of each star\n",
    "star_x = Restaurant_AZ_reviews.review_stars.value_counts().index\n",
    "star_y = Restaurant_AZ_reviews.review_stars.value_counts().values\n",
    "\n",
    "plot.figure(figsize=(8,5))\n",
    "# colors are in the order 5, 4, 3, 1, 2\n",
    "bar_colors = ['darkgreen', 'mediumseagreen', 'gold', 'crimson', 'orange']\n",
    "plot.bar(star_x, star_y, color=bar_colors, width=.6)\n",
    "plot.xlabel('Stars (Rating)')\n",
    "plot.ylabel('Number of Reviews')\n",
    "plot.title(f'Number of Reviews Per Rating of {businesses_to_analyse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we restrich Restaurants to this number to develop the code\n",
    "sample_restaurants_to_load = 10\n",
    "\n",
    "# Only Arizona Businesses, Change if needed\n",
    "restaurant_file='processed_data/restaurants_az.csv'\n",
    "reviews_file   ='processed_data/restaurant_az_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SPACY\n",
    "# This is the large Spacy English Library\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp2 = spacy.load('en_core_web_lg', disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords for topic mining\n",
    "stopwords = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words that appear in names of the Restaurants\n",
    "# Restaurants name may appear multiple time in review, increasing its word frequenty\n",
    "# For topic mining per restaurant, it is not useful and should be removed\n",
    "# However words such as 'chicken' when come in restaurant name should be retained\n",
    "stopnames = [line.rstrip('\\n').lower() for line in open('config/names.txt', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read Businesses\n",
    "all_restaurants = pd.read_csv(restaurant_file).drop(labels='Unnamed: 0', axis=1).head(sample_restaurants_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read all reviews\n",
    "all_reviews = pd.read_csv(reviews_file).drop(labels='Unnamed: 0', axis=1).drop(labels='city', axis=1).drop(labels='state', axis=1).drop(labels='categories', axis=1).drop(labels='user_id', axis=1).drop(labels='date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Retain reviews of selected Businesses\n",
    "all_reviews = all_reviews[all_reviews.business_id.isin(all_restaurants.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Top 5 Reviews\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docs(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String List cleaning, removes spaces, new lines\n",
    "def clean_string(data):\n",
    "    data = [re.sub('\\s+', ' '  , sent) for sent in data]\n",
    "    data = [re.sub(\"n't\", 'not', sent) for sent in data]\n",
    "    data = [sent.lower()               for sent in data]\n",
    "    data = list(tokenize_docs(data))\n",
    "    data = [[tok for tok in sent if tok not in stopwords ] for sent in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    name_toks = []\n",
    "    \n",
    "    # Nlp doc from Name\n",
    "    name_doc = nlp(name)\n",
    "    for token in name_doc:\n",
    "        \n",
    "        # Retain Proper nouns in Name\n",
    "        if token.pos_ == 'PROPN' or token.like_num:\n",
    "        \n",
    "            # Lose stop words in Name\n",
    "            if token.text.lower() not in stopnames:\n",
    "            \n",
    "                # All Restaurant name tokens to be remoed from reviews of this reataurant\n",
    "                name_toks.append(token.text.lower())\n",
    "    \n",
    "    #for noun_phrase in list(name_doc.noun_chunks):\n",
    "        #if(len(str(noun_phrase).split())<2):\n",
    "            #noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_)\n",
    "    \n",
    "    \n",
    "    for chunk in name_doc.ents:\n",
    "        name_toks.append(chunk.text.lower())\n",
    "    \n",
    "    return name_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_grams(ngram):\n",
    "    postags=['PROPN', 'NOUN']\n",
    "    text = ngram.replace('_',' ')\n",
    "    newgram = []\n",
    "    doc = nlp2(text)\n",
    "    index = 0\n",
    "    for tok in doc:\n",
    "        print(f\"{tok} {tok.pos_}\")\n",
    "        if index == 0 and tok.pos_ in postags:\n",
    "            index +=1\n",
    "        if index == 1 and tok.pos_ not in postags:\n",
    "            index +=1\n",
    "        if len(newgram) <= index:\n",
    "            newgram.append([])\n",
    "        pprint(newgram)\n",
    "        pprint(len(newgram[0]))\n",
    "        newgram[index].append(tok.text.lower())\n",
    "     \n",
    "    print(reduce(operator.concat, newgram))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spicy ADJ\n",
      "[[]]\n",
      "0\n",
      "hot ADJ\n",
      "[['spicy']]\n",
      "1\n",
      "dog NOUN\n",
      "[['spicy', 'hot'], []]\n",
      "2\n",
      "dish NOUN\n",
      "[['spicy', 'hot'], ['dog']]\n",
      "2\n",
      "['spicy', 'hot', 'dog', 'dish']\n"
     ]
    }
   ],
   "source": [
    "noun_grams('spicy_hot_dog_dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc,name_toks,allowed_postags=['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    # Remove punctuation, symbols (#) and stopwords\n",
    "    allowed_postags=['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "    toks = [tok.lemma_ for tok in doc ]\n",
    "    doc = nlp2(\" \".join(toks))\n",
    "    \n",
    "    [noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_) for noun_phrase in doc.noun_chunks if len(str(noun_phrase).split())>1 and len(str(noun_phrase).split())<4]\n",
    "    \n",
    "    #doc = [tok.text for tok in doc if (tok.text.lower() not in stopwords and tok.pos_ != \"PUNCT\" and tok.pos_ != \"SYM\")]\n",
    "    toks = [tok.text.lower().strip().replace('_',' ') for tok in doc \n",
    "                if (tok.text.lower().strip().replace('_',' ') not in stopwords \n",
    "                    and tok.text.lower().replace('_',' ') not in name_toks \n",
    "                    and tok.pos_ in allowed_postags\n",
    "                   )]\n",
    "    return \" \".join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total = len(all_restaurants)\n",
    "cleansed_text = []\n",
    "for index, restaurant in all_restaurants.iterrows():\n",
    "    #print(f'Cleaning reviews for restaurant: \"{restaurant[\"name\"]:<{40}}\" [{index+1:>{5}}/{total:>{5}}]')\n",
    "    if index % 100 == 0:\n",
    "        print(f'Cleaning reviews [{index+1:>{5}}/{total:>{5}}]')\n",
    "    \n",
    "    # Convert to list\n",
    "    data = all_reviews.query(' business_id == \"'+restaurant['business_id']+'\" ')['text']\n",
    "    \n",
    "    # Remove new lines, spaces, etc. Remove stopwords\n",
    "    data = clean_string(data)\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram  = gensim.models.Phrases(data, min_count=4, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data],min_count=3, threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod  = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    bigrams  = [bigram_mod[doc] for doc in data]\n",
    "    trigrams = [trigram_mod[bigram_mod[doc]] for doc in data]\n",
    "    \n",
    "    data = [\" \".join(trigram) for trigram in trigrams]\n",
    "    #data = [\" \".join(toks) for toks in data] \n",
    "    \n",
    "    # iterate list, clean sentences\n",
    "    for parsed_review in nlp.pipe(iter(data), batch_size=1000, n_threads=8):\n",
    "        #[noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_) for noun_phrase in parsed_review.noun_chunks if len(str(noun_phrase).split())>1 and len(str(noun_phrase).split())<4]\n",
    "        cleansed_text.append(clean_doc(parsed_review,clean_name(restaurant[\"name\"])))\n",
    "        pprint(cleansed_text[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews['cleansed_text'] = cleansed_text\n",
    "all_reviews.to_csv('processed_data/cleaned_reviews.csv')\n",
    "all_restaurants.to_csv('processed_data/cleaned_restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"manager family venue weddings_event gathering day table remember wait line time seat food prefer lunch dim_sum wheel cart pick stuff cart shrimp ball dumpling love plate scallop cook service cart lady speak break english bit vietnamese drink sauce plate ask people reason find dinner experience bit love lunch think fun notice drop quality goto,frequent_year manager pretty much_recognize family probably favorite chinese_lunch huge relative_book venue weddings_event gathering usually good_experience day busy_time. bad_wait. table remember wait line past_door time seat. maybe_change food. honestly_think great_bit expensive_offer prefer lunch. dim_sum wheel cart pick nice definitely decisive_person usually tasty_look stuff cart favorite_crap shrimp ball. dumpling good love green_vegetable plate scallop cook never_rubbery overcooked_meat. service. pretty good_lot cart lady speak break english bit vietnamese hard. usually fast_bring drink sauce plate ask right away definitely_recommend people reason find dinner experience bit disappointing. often_dish warm cool love lunch. think fun tasty_experience. actually almost_week notice drop quality probably_remain goto chinese_lunch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager NOUN\n",
      "family NOUN\n",
      "venue NOUN\n",
      "weddings_event NOUN\n",
      "gathering VERB\n",
      "day NOUN\n",
      "table NOUN\n",
      "remember VERB\n",
      "wait VERB\n",
      "line NOUN\n",
      "time NOUN\n",
      "seat NOUN\n",
      "food NOUN\n",
      "prefer VERB\n",
      "lunch NOUN\n",
      "dim_sum NOUN\n",
      "wheel NOUN\n",
      "cart NOUN\n",
      "pick VERB\n",
      "stuff NOUN\n",
      "cart NOUN\n",
      "shrimp NOUN\n",
      "ball NOUN\n",
      "dumpling NOUN\n",
      "love NOUN\n",
      "plate NOUN\n",
      "scallop NOUN\n",
      "cook NOUN\n",
      "service NOUN\n",
      "cart NOUN\n",
      "lady NOUN\n",
      "speak VERB\n",
      "break VERB\n",
      "english ADJ\n",
      "bit NOUN\n",
      "vietnamese ADJ\n",
      "drink NOUN\n",
      "sauce NOUN\n",
      "plate NOUN\n",
      "ask VERB\n",
      "people NOUN\n",
      "reason NOUN\n",
      "find VERB\n",
      "dinner NOUN\n",
      "experience NOUN\n",
      "bit NOUN\n",
      "love NOUN\n",
      "lunch NOUN\n",
      "think VERB\n",
      "fun NOUN\n",
      "notice NOUN\n",
      "drop NOUN\n",
      "quality NOUN\n",
      "goto NOUN\n",
      ", PUNCT\n",
      "frequent_year NOUN\n",
      "manager NOUN\n",
      "pretty ADV\n",
      "much_recognize ADJ\n",
      "family NOUN\n",
      "probably ADV\n",
      "favorite ADJ\n",
      "chinese_lunch ADJ\n",
      "huge ADJ\n",
      "relative_book NOUN\n",
      "venue NOUN\n",
      "weddings_event NOUN\n",
      "gathering NOUN\n",
      "usually ADV\n",
      "good_experience NOUN\n",
      "day NOUN\n",
      "busy_time NOUN\n",
      ". PUNCT\n",
      "bad_wait INTJ\n",
      ". PUNCT\n",
      "table NOUN\n",
      "remember VERB\n",
      "wait VERB\n",
      "line NOUN\n",
      "past_door ADJ\n",
      "time NOUN\n",
      "seat NOUN\n",
      ". PUNCT\n",
      "maybe_change VERB\n",
      "food NOUN\n",
      ". PUNCT\n",
      "honestly_think PROPN\n",
      "great_bit ADP\n",
      "expensive_offer NOUN\n",
      "prefer VERB\n",
      "lunch NOUN\n",
      ". PUNCT\n",
      "dim_sum NOUN\n",
      "wheel NOUN\n",
      "cart NOUN\n",
      "pick VERB\n",
      "nice ADJ\n",
      "definitely ADV\n",
      "decisive_person NOUN\n",
      "usually ADV\n",
      "tasty_look NOUN\n",
      "stuff NOUN\n",
      "cart NOUN\n",
      "favorite_crap NOUN\n",
      "shrimp NOUN\n",
      "ball NOUN\n",
      ". PUNCT\n",
      "dumpling VERB\n",
      "good ADJ\n",
      "love NOUN\n",
      "green_vegetable ADJ\n",
      "plate NOUN\n",
      "scallop NOUN\n",
      "cook NOUN\n",
      "never_rubbery PROPN\n",
      "overcooked_meat NOUN\n",
      ". PUNCT\n",
      "service NOUN\n",
      ". PUNCT\n",
      "pretty ADJ\n",
      "good_lot DET\n",
      "cart NOUN\n",
      "lady NOUN\n",
      "speak VERB\n",
      "break VERB\n",
      "english ADJ\n",
      "bit NOUN\n",
      "vietnamese ADJ\n",
      "hard ADV\n",
      ". PUNCT\n",
      "usually ADV\n",
      "fast_bring VERB\n",
      "drink NOUN\n",
      "sauce NOUN\n",
      "plate NOUN\n",
      "ask VERB\n",
      "right ADV\n",
      "away ADV\n",
      "definitely_recommend ADJ\n",
      "people NOUN\n",
      "reason NOUN\n",
      "find VERB\n",
      "dinner NOUN\n",
      "experience NOUN\n",
      "bit NOUN\n",
      "disappointing ADJ\n",
      ". PUNCT\n",
      "often_dish ADJ\n",
      "warm ADJ\n",
      "cool ADJ\n",
      "love NOUN\n",
      "lunch NOUN\n",
      ". PUNCT\n",
      "think VERB\n",
      "fun NOUN\n",
      "tasty_experience NOUN\n",
      ". PUNCT\n",
      "actually ADV\n",
      "almost_week VERB\n",
      "notice NOUN\n",
      "drop NOUN\n",
      "quality NOUN\n",
      "probably_remain NOUN\n",
      "goto NOUN\n",
      "chinese_lunch NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for tok in doc:\n",
    "    print(f'{tok.text} {tok.pos_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
