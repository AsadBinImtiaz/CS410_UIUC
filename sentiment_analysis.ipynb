{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "%matplotlib inline\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install missing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} s3fs seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_bucket(bucket, data_key):\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "    chunksize = 1000000\n",
    "    chunk_list = []\n",
    "    df_chunk = pd.read_csv(data_location, chunksize=chunksize)\n",
    "    for chunk in df_chunk:\n",
    "        chunk_list.append(chunk)\n",
    "\n",
    "    df = pd.concat(chunk_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 1 == 1:\n",
    "    \n",
    "    bucket = 'cs410-yelp'\n",
    "    data_key = 'processed_data/cleaned_reviews.csv'\n",
    "\n",
    "    #df = read_s3_bucket(bucket, data_key)\n",
    "    df = pd.read_csv('processed_data/cleaned_reviews.csv')\n",
    "    df = df.drop(labels='Unnamed: 0', axis=1)\n",
    "    df['review_stars']   = df['review_stars'].astype(int)\n",
    "    df['sentiment_text'] = df['sentiment_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3527902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>useful</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.374313</td>\n",
       "      <td>507.893546</td>\n",
       "      <td>1.431601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.529216</td>\n",
       "      <td>569.256905</td>\n",
       "      <td>1.338946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.633498</td>\n",
       "      <td>615.480560</td>\n",
       "      <td>1.189830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.790773</td>\n",
       "      <td>633.882706</td>\n",
       "      <td>1.206635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.983838</td>\n",
       "      <td>647.555980</td>\n",
       "      <td>0.896749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_stars  review_count    useful\n",
       "review_stars                                        \n",
       "1                   3.374313    507.893546  1.431601\n",
       "2                   3.529216    569.256905  1.338946\n",
       "3                   3.633498    615.480560  1.189830\n",
       "4                   3.790773    633.882706  1.206635\n",
       "5                   3.983838    647.555980  0.896749"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stval = df.groupby('review_stars').mean()\n",
    "stval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = frozenset(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473700, 10)\n",
      "3     horrible-service boyfriend because pass couple...\n",
      "14    big chinese-mississauga solid good-food. recen...\n",
      "25    time year definitely-change quality food gone-...\n",
      "39    family probably-close year. almost certain cha...\n",
      "41    unfortunately not-choice dimsum mississauga di...\n",
      "Name: sentiment_text, dtype: object\n",
      "3     1\n",
      "14    3\n",
      "25    1\n",
      "39    3\n",
      "41    3\n",
      "Name: review_stars, dtype: int32\n",
      "Wall time: 651 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CLASSIFICATION\n",
    "df_classes = df[(df['review_stars'] == 1) | (df['review_stars'] == 3) | (df['review_stars'] == 5)]\n",
    "df_classes = df_classes[(df_classes['useful'] == 1)]\n",
    "df_classes.head()\n",
    "print(df_classes.shape)\n",
    "\n",
    "# Seperate the data set into X and Y for prediction\n",
    "x = df_classes['sentiment_text']\n",
    "y = df_classes['review_stars']\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815552\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = CountVectorizer(analyzer=text_process,stop_words=stopwords).fit(x)\n",
    "print(len(vocab.vocabulary_))\n",
    "r0 = x[0]\n",
    "print(r0)\n",
    "vocab0 = vocab.transform([r0])\n",
    "print(vocab0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization of the whole review set and and checking the sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the sparse matrix: (473700, 815552)\n",
      "Non-Zero occurences: 14210164\n",
      "Density of the matrix: 0.003678273754455728\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = vocab.transform(x)\n",
    "#Shape of the matrix:\n",
    "print(\"Shape of the sparse matrix: {}\".format(x.shape))\n",
    "#Non-zero occurences:\n",
    "print(\"Non-Zero occurences: {}\".format(x.nnz))\n",
    "\n",
    "# DENSITY OF THE MATRIX\n",
    "density = (x.nnz / (x.shape[0] * x.shape[1])) * 100\n",
    "print(\"Density of the matrix: {}\".format(density))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data set into training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_pred, classifier_name):\n",
    "    print(\"Confusion Matrix for {}:\".format(classifier_name))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['1-Star', '3-Star', '5-Star']))\n",
    "    print(\"\\nScore: {}\".format(round(accuracy_score(y_true, y_pred)*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      "[[ 246   70   58]\n",
      " [  53  251  283]\n",
      " [   7   39 1360]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-Star       0.80      0.66      0.72       374\n",
      "      3-Star       0.70      0.43      0.53       587\n",
      "      5-Star       0.80      0.97      0.88      1406\n",
      "\n",
      "    accuracy                           0.78      2367\n",
      "   macro avg       0.77      0.68      0.71      2367\n",
      "weighted avg       0.77      0.78      0.77      2367\n",
      "\n",
      "\n",
      "Score: 78.45\n",
      "CPU times: user 21.2 ms, sys: 3.84 ms, total: 25.1 ms\n",
      "Wall time: 23.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train, y_train)\n",
    "predmnb = mnb.predict(x_test)\n",
    "print_results(y_test, predmnb, \"Multinomial Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/project/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Random Forest Classifier:\n",
      "[[ 188   65  121]\n",
      " [  78  155  354]\n",
      " [  45   81 1280]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-Star       0.60      0.50      0.55       374\n",
      "      3-Star       0.51      0.26      0.35       587\n",
      "      5-Star       0.73      0.91      0.81      1406\n",
      "\n",
      "    accuracy                           0.69      2367\n",
      "   macro avg       0.62      0.56      0.57      2367\n",
      "weighted avg       0.66      0.69      0.65      2367\n",
      "\n",
      "\n",
      "Score: 68.57\n",
      "CPU times: user 4.01 s, sys: 25.9 ms, total: 4.03 s\n",
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rmfr = RandomForestClassifier()\n",
    "rmfr.fit(x_train, y_train)\n",
    "p = rmfr.predict(x_test)\n",
    "print_results(y_test, p, \"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Decision Tree:\n",
      "[[ 183   89  102]\n",
      " [  89  211  287]\n",
      " [  91  158 1157]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-Star       0.50      0.49      0.50       374\n",
      "      3-Star       0.46      0.36      0.40       587\n",
      "      5-Star       0.75      0.82      0.78      1406\n",
      "\n",
      "    accuracy                           0.66      2367\n",
      "   macro avg       0.57      0.56      0.56      2367\n",
      "weighted avg       0.64      0.66      0.64      2367\n",
      "\n",
      "\n",
      "Score: 65.53\n",
      "CPU times: user 4.41 s, sys: 14 ms, total: 4.43 s\n",
      "Wall time: 4.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "p = dt.predict(x_test)\n",
    "print_results(y_test, p, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/project/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SVM:\n",
      "[[   0    0  374]\n",
      " [   0    0  587]\n",
      " [   0    0 1406]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-Star       0.00      0.00      0.00       374\n",
      "      3-Star       0.00      0.00      0.00       587\n",
      "      5-Star       0.59      1.00      0.75      1406\n",
      "\n",
      "    accuracy                           0.59      2367\n",
      "   macro avg       0.20      0.33      0.25      2367\n",
      "weighted avg       0.35      0.59      0.44      2367\n",
      "\n",
      "\n",
      "Score: 59.4\n",
      "CPU times: user 22.5 s, sys: 200 ms, total: 22.7 s\n",
      "Wall time: 22.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/project/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state=101)\n",
    "svm.fit(x_train,y_train)\n",
    "p = svm.predict(x_test)\n",
    "print_results(y_test, p, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for kNN:\n",
      "[[238  17 119]\n",
      " [212  48 327]\n",
      " [382  26 998]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-Star       0.29      0.64      0.39       374\n",
      "      3-Star       0.53      0.08      0.14       587\n",
      "      5-Star       0.69      0.71      0.70      1406\n",
      "\n",
      "    accuracy                           0.54      2367\n",
      "   macro avg       0.50      0.48      0.41      2367\n",
      "weighted avg       0.59      0.54      0.51      2367\n",
      "\n",
      "\n",
      "Score: 54.25\n",
      "CPU times: user 717 ms, sys: 169 ms, total: 886 ms\n",
      "Wall time: 887 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(x_train,y_train)\n",
    "p = knn.predict(x_test)\n",
    "print_results(y_test, p, \"kNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(x_train,y_train)\n",
    "p = mlp.predict(x_test)\n",
    "print_results(y_test, p, \"Multilayer Perceptron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 11\n",
    "pr = df['sentiment_text'][item]\n",
    "print(pr)\n",
    "print(\"\\nActual Rating: {}\".format(df['review_stars'][item]))\n",
    "pr_t = vocab.transform([pr])\n",
    "print(\"Predicted Rating: {}\".format(mlp.predict(pr_t)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
