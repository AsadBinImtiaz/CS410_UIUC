{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed libs\n",
    "import gensim \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_topic_data():\n",
    "    return pd.read_csv('processed_data/cleaned_reviews.csv', usecols = [\"business_id\", \"name\", \"review_id\",\"text\",\"topic_text\",\"sentiment_text\",\"review_stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_topic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')] \n",
    "neg_words  = [line.rstrip('\\n') for line in open('config/negations.txt', 'r', encoding='utf-8')]\n",
    "stopwords = stop_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(list1,list2):\n",
    "    return list(itertools.filterfalse(set(list2).__contains__, list1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_phrases(str_list):\n",
    "    new_list = []\n",
    "    for tok in str_list:\n",
    "        if '-' in tok:\n",
    "            new_list += tok.split('-')\n",
    "        new_list.append(tok)\n",
    "    return list_diff(new_list,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_set = [split_phrases((u''+str(txt)).replace('.','').split()) for txt in data.topic_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_set = [split_phrases((u''+str(txt)).replace('.','').split()) for txt in data.sentiment_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#build vocabulary and train model\n",
    "topic_model = gensim.models.Word2Vec(\n",
    "        topic_set,\n",
    "        size=100,\n",
    "        window=10,\n",
    "        min_count=100,\n",
    "        workers=100,\n",
    "        iter=10)\n",
    "\n",
    "#  Stats\n",
    "## NC (0250 MB): Wall time:   14.8 s /   22 s\n",
    "## ALL         : 4min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#build vocabulary and train model\n",
    "senti_model = gensim.models.Word2Vec(\n",
    "        senti_set,\n",
    "        size=100,\n",
    "        window=15,\n",
    "        min_count=100,\n",
    "        workers=100,\n",
    "        iter=20)\n",
    "\n",
    "#  Stats\n",
    "## NC (0250 MB): Wall time:   14.8 s /   22 s\n",
    "## ALL         : Wall time: 17min 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/aspect_topic_model.pk', 'wb') as fin:\n",
    "#    pickle.dump(topic_model, fin)\n",
    "#with open('pickles/aspect_senti_model.pk', 'wb') as fin:\n",
    "#    pickle.dump(senti_model, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-876f935414ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'restaurant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "stopwords.remove('restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model = None\n",
    "topic_model = None\n",
    "with open('pickles/aspect_topic_model.pk', 'rb') as fin:\n",
    "    topic_model = pickle.load(fin)\n",
    "with open('pickles/aspect_senti_model.pk', 'rb') as fin:\n",
    "    senti_model = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dana_park', 0.6781737208366394),\n",
       " ('warm_spre', 0.6394859552383423),\n",
       " ('pv_mall', 0.6201270222663879),\n",
       " ('stapley', 0.6192067265510559),\n",
       " ('greenway', 0.6060971021652222)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.wv.most_similar(positive='location',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superior-food', 0.6407245993614197),\n",
       " ('superior-service', 0.5888616442680359)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_model.wv.most_similar(positive='far',topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22458845"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_model.wv.similarity(w1='location',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016833797"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_model.wv.similarity(w1='staff',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042283814"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_model.wv.similarity(w1='atmosphere',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050098836"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_model.wv.similarity(w1='price',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07305616"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_model.wv.similarity(w1='service',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aspect_Terms = ['food','service','staff','location','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_topics_map():\n",
    "    with open('config/all_topics.txt', 'r') as f:\n",
    "         ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = get_all_topics_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for x in topic_map.values():\n",
    "    for y in x.split('/'):\n",
    "        vocab.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_map():\n",
    "    with open('config/aspect_map.txt', 'r') as f:\n",
    "        ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_map = get_aspect_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['food', 'atmosphere', 'staff', 'value', 'cent', 'location', 'appetizer', 'area', 'attentive', 'bacon', 'bar', 'bartender', 'beer', 'bill', 'bland', 'bread', 'breakfast', 'brunch', 'buffet', 'burger', 'burrito', 'caesar', 'charge', 'check', 'cheese', 'chef', 'chicken', 'clean', 'coffee', 'cook', 'couple', 'crust', 'customer', 'service', 'delicious', 'dessert', 'dinner', 'dish', 'dollar', 'dress', 'drink', 'drive', 'employee', 'fast', 'fish', 'flavor', 'friendly', 'friendly-staff', 'great-atmosphere', 'great-customer', 'great-food', 'great-service', 'grill', 'happy-hour', 'helpful', 'hotel', 'hour', 'inside', 'lobster', 'lunch', 'manager', 'meal', 'meat', 'minute', 'noodle', 'not-wait', 'offer', 'option', 'outside', 'pancake', 'pasta', 'people', 'pizza', 'portion', 'price', 'bad', 'overprice', 'money', 'quality', 'refill', 'reservation', 'review', 'selection', 'server', 'slow', 'special', 'star', 'street', 'tasty', 'town', 'parking', 'kitchen', 'table', 'chair', 'vegas', 'visit', 'wait', 'waiter', 'wine', 'wing', 'yummy', 'job', 'place', 'door', 'Ingredient', 'lady', 'attitude', 'ambience', 'environment', 'rude', 'toilet', 'washroom', 'city', 'downtown'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_aspect(sent,aspect_map):\n",
    "    dicty = {}\n",
    "    \n",
    "    for word in nlp(sent.lower()):\n",
    "        if word.lemma_ not in stopwords:\n",
    "            score = 0\n",
    "            aspec = None\n",
    "            for asp in aspect_map.keys():\n",
    "                try:\n",
    "                    sco = senti_model.wv.similarity(w1=asp,w2=word.lemma_)\n",
    "                    if sco > score:\n",
    "                        score=sco\n",
    "                        aspec=asp\n",
    "                        dicty[aspec]= dicty.setdefault(asp, 0) + score    \n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "    if len(dicty)==0:\n",
    "        return None\n",
    "    \n",
    "    ret = max(dicty.items(), key=lambda k: k[1])\n",
    "    if ret[1]<0.3:\n",
    "        return None\n",
    "    \n",
    "    return aspect_map[ret[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......... Clustered [2260001/3527902 ] -    2223.6 secs\n",
      ".......... Clustered [2270001/3527902 ] -    3284.8 secs\n",
      ".......... Clustered [2280001/3527902 ] -    2046.9 secs\n",
      ".......... Clustered [2290001/3527902 ] -    1981.5 secs\n",
      ".......... Clustered [2300001/3527902 ] -    1826.4 secs\n",
      ".......... Clustered [2310001/3527902 ] -    1800.4 secs\n",
      ".......... Clustered [2320001/3527902 ] -    1703.6 secs\n",
      ".......... Clustered [2330001/3527902 ] -    1841.2 secs\n",
      ".......... Clustered [2340001/3527902 ] -    1809.3 secs\n",
      ".......... Clustered [2350001/3527902 ] -    1886.8 secs\n",
      ".......... Clustered [2360001/3527902 ] -    2033.6 secs\n",
      ".......... Clustered [2370001/3527902 ] -    1913.6 secs\n",
      ".......... Clustered [2380001/3527902 ] -    1719.1 secs\n",
      ".......... Clustered [2390001/3527902 ] -    1852.0 secs\n",
      ".......... Clustered [2400001/3527902 ] -    2061.2 secs\n",
      ".......... Clustered [2410001/3527902 ] -    1688.8 secs\n",
      ".......... Clustered [2420001/3527902 ] -    1759.9 secs\n",
      ".......... Clustered [2430001/3527902 ] -    1761.0 secs\n",
      ".......... Clustered [2440001/3527902 ] -    1659.7 secs\n",
      ".......... Clustered [2450001/3527902 ] -    1836.2 secs\n",
      ".......... Clustered [2460001/3527902 ] -    1641.1 secs\n",
      ".......... Clustered [2470001/3527902 ] -    1670.6 secs\n",
      ".......... Clustered [2480001/3527902 ] -    1714.9 secs\n",
      ".......... Clustered [2490001/3527902 ] -    1685.9 secs\n",
      ".......... Clustered [2500001/3527902 ] -    1904.6 secs\n",
      ".......... Clustered [2510001/3527902 ] -    1914.8 secs\n",
      ".......... Clustered [2520001/3527902 ] -    1993.2 secs\n",
      ".......... Clustered [2530001/3527902 ] -    2035.9 secs\n",
      ".......... Clustered [2540001/3527902 ] -    1958.4 secs\n",
      ".......... Clustered [2550001/3527902 ] -    1809.7 secs\n",
      ".......... Clustered [2560001/3527902 ] -    2095.4 secs\n",
      ".......... Clustered [2570001/3527902 ] -    2201.6 secs\n",
      ".......... Clustered [2580001/3527902 ] -    2185.6 secs\n",
      ".......... Clustered [2590001/3527902 ] -    2255.7 secs\n",
      ".......... Clustered [2600001/3527902 ] -    1833.0 secs\n",
      ".......... Clustered [2610001/3527902 ] -    2181.6 secs\n",
      ".......... Clustered [2620001/3527902 ] -    2098.1 secs\n",
      "......."
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "total = len(data)\n",
    "for index,txt in enumerate(data.text):\n",
    "    if index < 2250001:\n",
    "        continue;\n",
    "        \n",
    "    mydict = {}\n",
    "    for w in Aspect_Terms:\n",
    "        mydict[w] = mydict.setdefault(w, '')  \n",
    "    for x in txt.replace('\\\\n','.').replace('\\n','.').replace('restaurant','location').split('.'):\n",
    "        if(len(x)>0):\n",
    "            y = assign_aspect(x,aspect_map)\n",
    "            mydict[y] = mydict.setdefault(y, '') + x +'.\\n' \n",
    "    mydict.pop(None, None)\n",
    "    lst.append(mydict)\n",
    "    \n",
    "    if index%1000 == 0 and index > 0:\n",
    "        print(f\".\", end='')\n",
    "        \n",
    "    if index%10000 == 0 and index > 0:\n",
    "        end = time.time()\n",
    "        print(f' Clustered [{index+1:>{5}}/{total:>{5}} ] - {str(end-start):>{9.6}} secs')\n",
    "        start = time.time()\n",
    "        with open('pickles/'+str(index)+'.pk', 'wb') as fin:\n",
    "            pickle.dump(lst, fin)\n",
    "        lst = []\n",
    "        \n",
    "print(f'Clustered [{total:>{5}}] - {str(end-start):>{9.6}} secs')\n",
    "\n",
    "with open('pickles/3530000.pk', 'wb') as fin:\n",
    "    pickle.dump(lst, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for index in range(len(data)):\n",
    "    if index%10000 == 0 and index > 0:\n",
    "        with open('pickles/'+index+'.pk', 'rb') as fin:\n",
    "            lst += pickle.load(fin)\n",
    "with open('pickles/353.pk', 'rb') as fin:\n",
    "    lst += pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_df = pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df = data[['review_id','text','review_stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asp in Aspect_Terms:\n",
    "    aspects_df[asp] = aspect_df[asp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df.to_csv('aspects_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
