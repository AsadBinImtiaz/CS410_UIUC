{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libs\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# Mongo DB\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from MongoDB\n",
    "DBClient = MongoClient()\n",
    "yelp_data = DBClient.yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select business having atleast 50 reviews\n",
    "min_review_count = 50\n",
    "\n",
    "# businesses to Analyse\n",
    "businesses_to_analyse = 'Restaurants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all restaurant businesses\n",
    "Restaurant_business = pd.DataFrame(yelp_data.business.find({\"categories\":{\"$regex\" :\".*\"+businesses_to_analyse+\".*\"}, \"review_count\":{\"$gte\":min_review_count} },  {'business_id':1, 'name':1, 'city':1, 'state':1, 'stars':1, 'review_count':1, 'categories':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all reviews\n",
    "All_reviews = pd.DataFrame(yelp_data.review.find({},{'review_id':1, 'user_id':1, 'business_id':1, 'stars':1, 'useful':1, 'text':1, 'date':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all restaurant reviews\n",
    "#Restaurant_reviews = All_reviews[All_reviews.business_id.isin(Restaurant_business.business_id.values)]\n",
    "Restaurant_reviews = pd.merge(Restaurant_business,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 Restaurant\n",
    "Restaurant_business.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 Reviews\n",
    "Restaurant_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected Restaurants to file\n",
    "Restaurant_reviews.to_csv('processed_data/restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected Restaurant-reviews to file\n",
    "Restaurant_business.to_csv('processed_data/restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many reviews we have of each star\n",
    "star_x = Restaurant_reviews.review_stars.value_counts().index\n",
    "star_y = Restaurant_reviews.review_stars.value_counts().values\n",
    "\n",
    "plot.figure(figsize=(8,5))\n",
    "# colors are in the order 5, 4, 3, 1, 2\n",
    "bar_colors = ['darkgreen', 'mediumseagreen', 'gold', 'crimson', 'orange']\n",
    "plot.bar(star_x, star_y, color=bar_colors, width=.6)\n",
    "plot.xlabel('Stars (Rating)')\n",
    "plot.ylabel('Number of Reviews')\n",
    "plot.title(f'Number of Reviews Per Rating of {businesses_to_analyse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_business.groupby('state').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_per_state = Restaurant_business.groupby('state').count()[['business_id']].rename(columns={'state': 'State', 'business_id': 'Restaurants'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_per_state.sort_values(by='Restaurants').plot.bar(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ = pd.DataFrame(yelp_data.business.find({\"categories\":{\"$regex\" :\".*\"+businesses_to_analyse+\".*\"}, \"review_count\":{\"$gte\":min_review_count}, \"state\":\"AZ\" },  {'business_id':1, 'name':1, 'city':1, 'state':1, 'stars':1, 'review_count':1, 'categories':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews = pd.merge(Restaurant_AZ,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews.to_csv('processed_data/restaurant_az_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ.to_csv('processed_data/restaurants_az.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many reviews we have of each star\n",
    "star_x = Restaurant_AZ_reviews.review_stars.value_counts().index\n",
    "star_y = Restaurant_AZ_reviews.review_stars.value_counts().values\n",
    "\n",
    "plot.figure(figsize=(8,5))\n",
    "# colors are in the order 5, 4, 3, 1, 2\n",
    "bar_colors = ['darkgreen', 'mediumseagreen', 'gold', 'crimson', 'orange']\n",
    "plot.bar(star_x, star_y, color=bar_colors, width=.6)\n",
    "plot.xlabel('Stars (Rating)')\n",
    "plot.ylabel('Number of Reviews')\n",
    "plot.title(f'Number of Reviews Per Rating of {businesses_to_analyse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we restrich Restaurants to this number to develop the code\n",
    "sample_restaurants_to_load = 10\n",
    "\n",
    "# Only Arizona Businesses, Change if needed\n",
    "restaurant_file='processed_data/restaurants_az.csv'\n",
    "reviews_file   ='processed_data/restaurant_az_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SPACY\n",
    "# This is the large Spacy English Library\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp2 = spacy.load('en_core_web_lg', disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords for topic mining\n",
    "stopwords = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words that appear in names of the Restaurants\n",
    "# Restaurants name may appear multiple time in review, increasing its word frequenty\n",
    "# For topic mining per restaurant, it is not useful and should be removed\n",
    "# However words such as 'chicken' when come in restaurant name should be retained\n",
    "stopnames = [line.rstrip('\\n').lower() for line in open('config/names.txt', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read Businesses\n",
    "all_restaurants = pd.read_csv(restaurant_file).drop(labels='Unnamed: 0', axis=1).head(sample_restaurants_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read all reviews\n",
    "all_reviews = pd.read_csv(reviews_file).drop(labels='Unnamed: 0', axis=1).drop(labels='city', axis=1).drop(labels='state', axis=1).drop(labels='categories', axis=1).drop(labels='user_id', axis=1).drop(labels='date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Retain reviews of selected Businesses\n",
    "all_reviews = all_reviews[all_reviews.business_id.isin(all_restaurants.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Top 5 Reviews\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docs(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String List cleaning, removes spaces, new lines\n",
    "def clean_string(data):\n",
    "    data = [re.sub('\\s+', ' '  , sent) for sent in data]\n",
    "    data = [re.sub(\"n't\", 'not', sent) for sent in data]\n",
    "    data = [sent.lower()               for sent in data]\n",
    "    data = list(tokenize_docs(data))\n",
    "    data = [[tok for tok in sent if tok not in stopwords ] for sent in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    name_toks = []\n",
    "    \n",
    "    # Nlp doc from Name\n",
    "    name_doc = nlp(name)\n",
    "    for token in name_doc:\n",
    "        \n",
    "        # Retain Proper nouns in Name\n",
    "        if token.pos_ == 'PROPN' or token.like_num:\n",
    "        \n",
    "            # Lose stop words in Name\n",
    "            if token.text.lower() not in stopnames:\n",
    "            \n",
    "                # All Restaurant name tokens to be remoed from reviews of this reataurant\n",
    "                name_toks.append(token.text.lower())\n",
    "    \n",
    "    #for noun_phrase in list(name_doc.noun_chunks):\n",
    "        #if(len(str(noun_phrase).split())<2):\n",
    "            #noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_)\n",
    "    \n",
    "    \n",
    "    for chunk in name_doc.ents:\n",
    "        name_toks.append(chunk.text.lower())\n",
    "    \n",
    "    return name_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_grams(ngram):\n",
    "    postags=['PROPN', 'NOUN']\n",
    "    text = ngram.replace('_',' ')\n",
    "    newgram = []\n",
    "    doc = nlp2(text)\n",
    "    index = 0\n",
    "    for tok in doc:\n",
    "        print(f\"{tok} {tok.pos_}\")\n",
    "        if index == 0 and tok.pos_ in postags:\n",
    "            index +=1\n",
    "        if index == 1 and tok.pos_ not in postags:\n",
    "            index +=1\n",
    "        if len(newgram) <= index:\n",
    "            newgram.append([])\n",
    "        pprint(newgram)\n",
    "        pprint(len(newgram[0]))\n",
    "        newgram[index].append(tok.text.lower())\n",
    "     \n",
    "    print(reduce(operator.concat, newgram))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spicy ADJ\n",
      "[[]]\n",
      "0\n",
      "hot ADJ\n",
      "[['spicy']]\n",
      "1\n",
      "dog NOUN\n",
      "[['spicy', 'hot'], []]\n",
      "2\n",
      "dish NOUN\n",
      "[['spicy', 'hot'], ['dog']]\n",
      "2\n",
      "['spicy', 'hot', 'dog', 'dish']\n"
     ]
    }
   ],
   "source": [
    "noun_grams('spicy_hot_dog_dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc,name_toks,allowed_postags=['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    # Remove punctuation, symbols (#) and stopwords\n",
    "    allowed_postags=['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "    toks = [tok.lemma_ for tok in doc ]\n",
    "    doc = nlp2(\" \".join(toks))\n",
    "    \n",
    "    [noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_) for noun_phrase in doc.noun_chunks if len(str(noun_phrase).split())>1 and len(str(noun_phrase).split())<4]\n",
    "    \n",
    "    #doc = [tok.text for tok in doc if (tok.text.lower() not in stopwords and tok.pos_ != \"PUNCT\" and tok.pos_ != \"SYM\")]\n",
    "    toks = [tok.text.lower().strip().replace('_',' ') for tok in doc \n",
    "                if (tok.text.lower().strip().replace('_',' ') not in stopwords \n",
    "                    and tok.text.lower().replace('_',' ') not in name_toks \n",
    "                    and tok.pos_ in allowed_postags\n",
    "                   )]\n",
    "    return \" \".join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total = len(all_restaurants)\n",
    "cleansed_text = []\n",
    "for index, restaurant in all_restaurants.iterrows():\n",
    "    #print(f'Cleaning reviews for restaurant: \"{restaurant[\"name\"]:<{40}}\" [{index+1:>{5}}/{total:>{5}}]')\n",
    "    if index % 100 == 0:\n",
    "        print(f'Cleaning reviews [{index+1:>{5}}/{total:>{5}}]')\n",
    "    \n",
    "    # Convert to list\n",
    "    data = all_reviews.query(' business_id == \"'+restaurant['business_id']+'\" ')['text']\n",
    "    \n",
    "    # Remove new lines, spaces, etc. Remove stopwords\n",
    "    data = clean_string(data)\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram  = gensim.models.Phrases(data, min_count=4, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data],min_count=3, threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod  = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    bigrams  = [bigram_mod[doc] for doc in data]\n",
    "    trigrams = [trigram_mod[bigram_mod[doc]] for doc in data]\n",
    "    \n",
    "    data = [\" \".join(trigram) for trigram in trigrams]\n",
    "    #data = [\" \".join(toks) for toks in data] \n",
    "    \n",
    "    # iterate list, clean sentences\n",
    "    for parsed_review in nlp.pipe(iter(data), batch_size=1000, n_threads=8):\n",
    "        #[noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_) for noun_phrase in parsed_review.noun_chunks if len(str(noun_phrase).split())>1 and len(str(noun_phrase).split())<4]\n",
    "        cleansed_text.append(clean_doc(parsed_review,clean_name(restaurant[\"name\"])))\n",
    "        pprint(cleansed_text[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews['cleansed_text'] = cleansed_text\n",
    "all_reviews.to_csv('processed_data/cleaned_reviews.csv')\n",
    "all_restaurants.to_csv('processed_data/cleaned_restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
