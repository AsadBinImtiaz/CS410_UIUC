{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libs\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint\n",
    "from functools import reduce \n",
    "import operator\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# S3 Access\n",
    "import s3fs\n",
    "\n",
    "# Mongo DB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pickle as pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SPACY\n",
    "# This is the large Spacy English Library\n",
    "nlp  = spacy.load('en_core_web_lg')\n",
    "nlp2 = spacy.load('en_core_web_lg', disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "We have loaded the data in *Mongo DB* (with pandas we can also read jsons directly, but it needs **10x** more time)\n",
    "From from the mongo DB, we select relevant data and write in csv files to be used for next steps\n",
    "\n",
    "The selection goes like this:\n",
    "* On restaurant businesses\n",
    "* Only Busenesses with 50 or more reviews \n",
    "* Select only following fields from businesses:\n",
    "    - 'business_id'\n",
    "    - 'name'\n",
    "    - 'city'\n",
    "    - 'state'\n",
    "    - 'stars'\n",
    "    - 'review_count'\n",
    "    - 'categories'\n",
    "* Select only following fields from reviews:\n",
    "    - 'review_id'\n",
    "    - 'user_id'\n",
    "    - 'business_id'\n",
    "    - 'stars'\n",
    "    - 'useful'\n",
    "    - 'text'\n",
    "    - 'date'\n",
    "    \n",
    "In the following, we connect to MongoDb and load data in Pandas Data Frames. The data from Restaurants and Reviews is merged in a single file, called **restaurant_reviews.csv**. All qualifying restaurant data is written in **restaurants.csv** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from MongoDB\n",
    "DBClient = MongoClient()\n",
    "yelp_data = DBClient.yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select business having atleast 50 reviews\n",
    "min_review_count = 50\n",
    "\n",
    "# businesses to Analyse\n",
    "businesses_to_analyse = 'Restaurants'\n",
    "\n",
    "state_filter = 'IL'\n",
    "\n",
    "# S3 Bucket\n",
    "access_key_id=\"AKIAS6LZOC5VADNJTXS7\"\n",
    "secret_key_id=\"aNV7W7oWviWop7+HZKr6RCSUVJ7QCyw6wSYxhI9L\"\n",
    "bucket_arn_id=\"cs410-yelp/\"\n",
    "bucket_region=\"N. Virginia\"\n",
    "\n",
    "bucket =  's3://'+bucket_arn_id\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key_id\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key_id\n",
    "os.environ['AWS_DEFAULT_REGION']='us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get all restaurant businesses\n",
    "Restaurant_business = pd.DataFrame(yelp_data.business.find({\"categories\":{\"$regex\" :\".*\"+businesses_to_analyse+\".*\"}, \"review_count\":{\"$gte\":min_review_count} },  {'business_id':1, 'name':1, 'city':1, 'state':1, 'stars':1, 'review_count':1, 'categories':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get all reviews\n",
    "All_reviews = pd.DataFrame(yelp_data.review.find({},{'review_id':1, 'user_id':1, 'business_id':1, 'stars':1, 'useful':1, 'text':1, 'date':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Find all restaurant reviews\n",
    "#Restaurant_reviews = All_reviews[All_reviews.business_id.isin(Restaurant_business.business_id.values)]\n",
    "Restaurant_reviews = pd.merge(Restaurant_business,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sample 5 Restaurant\n",
    "Restaurant_business.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert text to Unicode\n",
    "Restaurant_reviews['text'] = Restaurant_reviews['text'].map(lambda x: x.encode('unicode-escape','strict').decode('utf-8').replace('\\\\u',''))\n",
    "\n",
    "#Restaurant_reviews['text'] = Restaurant_reviews['text'].map(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', x.encode('unicode-escape','strict').decode('utf-8')).replace('\\\\u',''))\n",
    "#Restaurant_reviews['text'] = Restaurant_reviews[u'text'].map(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', x.encode('ascii', 'ignore').decode('utf-8')))\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 8.3 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert name to Unicode\n",
    "Restaurant_reviews['name']  = Restaurant_reviews['name'].map(lambda x: x.encode('unicode-escape','strict').decode('utf-8').replace('\\\\u',''))\n",
    "Restaurant_business['name'] = Restaurant_business['name'].map(lambda x: x.encode('unicode-escape','strict').decode('utf-8').replace('\\\\u',''))\n",
    "\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 2.63 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 Reviews\n",
    "Restaurant_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 Restaurants\n",
    "Restaurant_business.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected Restaurants to file\n",
    "Restaurant_reviews.to_csv('processed_data/restaurant_reviews.csv',encoding='utf-8',line_terminator='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected Restaurant-reviews to file\n",
    "Restaurant_business.to_csv('processed_data/restaurants.csv',encoding='utf-8',line_terminator='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some charts on the loaded data, just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many reviews we have of each star\n",
    "star_x = Restaurant_reviews.review_stars.value_counts().index\n",
    "star_y = Restaurant_reviews.review_stars.value_counts().values\n",
    "\n",
    "plot.figure(figsize=(8,5))\n",
    "# colors are in the order 5, 4, 3, 1, 2\n",
    "bar_colors = ['darkgreen', 'mediumseagreen', 'gold', 'crimson', 'orange']\n",
    "plot.bar(star_x, star_y, color=bar_colors, width=.6)\n",
    "plot.xlabel('Stars (Rating)')\n",
    "plot.ylabel('Number of Reviews')\n",
    "plot.title(f'Number of Reviews Per Rating of {businesses_to_analyse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakdown of restaurants per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_business.groupby('state').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filter = 'WI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_per_state = Restaurant_business.groupby('state').count()[['business_id']].rename(columns={'state': 'State', 'business_id': 'Restaurants'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_per_state.sort_values(by='Restaurants').plot.bar(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ = pd.DataFrame(yelp_data.business.find({\"categories\":{\"$regex\" :\".*\"+businesses_to_analyse+\".*\"}, \"review_count\":{\"$gte\":min_review_count}, \"state\":state_filter },  {'business_id':1, 'name':1, 'city':1, 'state':1, 'stars':1, 'review_count':1, 'categories':1, '_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews = pd.merge(Restaurant_AZ,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})\n",
    "Restaurant_AZ_reviews['text'] = Restaurant_AZ_reviews[u'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews['text']  = Restaurant_AZ_reviews['text'].map(lambda x: x.encode('unicode-escape','strict').decode('utf-8').replace('\\\\u',''))\n",
    "Restaurant_AZ_reviews['name']  = Restaurant_AZ_reviews['name'].map(lambda x: x.encode('unicode-escape','strict').decode('utf-8').replace('\\\\u',''))\n",
    "Restaurant_AZ['name'] = Restaurant_AZ['name'].map(lambda x: x.encode('unicode-escape','strict').decode('utf-8').replace('\\\\u',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews.to_csv('processed_data/'+state_filter+'_restaurant_reviews.csv',encoding='utf-8',line_terminator='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ.to_csv('processed_data/'+state_filter+'_restaurants.csv',encoding='utf-8',line_terminator='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_AZ_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many reviews we have of each star\n",
    "star_x = Restaurant_AZ_reviews.review_stars.value_counts().index\n",
    "star_y = Restaurant_AZ_reviews.review_stars.value_counts().values\n",
    "\n",
    "plot.figure(figsize=(8,5))\n",
    "# colors are in the order 5, 4, 3, 1, 2\n",
    "bar_colors = ['darkgreen', 'mediumseagreen', 'gold', 'crimson', 'orange']\n",
    "plot.bar(star_x, star_y, color=bar_colors, width=.6)\n",
    "plot.xlabel('Stars (Rating)')\n",
    "plot.ylabel('Number of Reviews')\n",
    "plot.title(f'Number of Reviews Per Rating of {businesses_to_analyse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Parsing\n",
    "\n",
    "Read preprocessed restaurant and Review files.\n",
    "For testing, we read only the restaurants and reviews in **Arizona**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we restrich Restaurants to this number to develop the code\n",
    "sample_restaurants_to_load = 100000\n",
    "\n",
    "# Only Arizona Businesses, Change if needed\n",
    "restaurant_file='processed_data/restaurants.csv'\n",
    "reviews_file   ='processed_data/restaurant_reviews.csv'\n",
    "#restaurant_file='processed_data/'+state_filter+'_restaurants.csv'\n",
    "#reviews_file   ='processed_data/'+state_filter+'_restaurant_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SPACY\n",
    "# This is the large Spacy English Library\n",
    "#nlp  = spacy.load('en_core_web_lg')\n",
    "#nlp2 = spacy.load('en_core_web_lg', disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All stopword in restaurant reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords for topic mining\n",
    "stopwords = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All negations in restaurant reviews to be merged as phrases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negations = [line.rstrip('\\n') for line in open('config/negations.txt', 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All stopword in restaurant names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words that appear in names of the Restaurants\n",
    "# Restaurants name may appear multiple time in review, increasing its word frequenty\n",
    "# For topic mining per restaurant, it is not useful and should be removed\n",
    "# However words such as 'chicken' when come in restaurant name should be retained\n",
    "stopnames = [line.rstrip('\\n').lower() for line in open('config/names.txt', 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Difference between 2 lists: list1 - List2\n",
    "# \n",
    "def list_diff(list1,list2):\n",
    "    return list(itertools.filterfalse(set(list2).__contains__, list1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Remove Stop Words\n",
    "# \n",
    "def remove_stop_words(data):\n",
    "    return [list_diff(sent,stopwords) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Remove List Words\n",
    "# \n",
    "def remove_list_words(data, alist):\n",
    "    return [list_diff(sent, alist) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Remove URLs from list of data\n",
    "#    \n",
    "def remove_urls (data):\n",
    "    start = time.time()\n",
    "    data = [re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ', str(sent).lower() , flags=re.MULTILINE) for sent in data]\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Remove new lies and symbols & lowercase from list of data\n",
    "#\n",
    "def remove_newlines(data):\n",
    "    start = time.time()\n",
    "    data = [str(sent).replace('\\\\n',' ').replace('\\n',' ').replace('.',' . ').replace(',',' , ').replace('?',' . ').replace('!',' . ') for sent in data]\n",
    "    data = [str(sent).replace(';',' . ').replace('\\r',' ').replace(':',' . ').replace('/',' / ').replace('\"','').replace('$',' dollars ') for sent in data]\n",
    "    data = [str(sent).replace('~','').replace('(','').replace(')','').replace('+','').replace('#','').replace('-','_').replace('%',' dollars ') for sent in data]\n",
    "    data = [str(sent).strip('*').strip('-').replace('=',' ').replace('@',' ').replace('^',' ') for sent in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Remove spaces and symbols from list of data\n",
    "#\n",
    "def remove_spaces (data):\n",
    "    start = time.time()\n",
    "    data = [re.sub('\\s+', ' '  ,  str(sent)) for sent in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Convert n't to not in list of data\n",
    "#\n",
    "def remove_short_nots (data):\n",
    "    start = time.time()\n",
    "    data = [re.sub(\"n't\", ' not', str(sent)) for sent in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# tokenize within list of data\n",
    "#\n",
    "def split_on_space (data):\n",
    "    start = time.time()\n",
    "    data = [sent.split() for sent in data]\n",
    "    #data = list(tokenize_docs(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read Businesses\n",
    "all_restaurants = pd.read_csv(restaurant_file).drop(labels='Unnamed: 0', axis=1).head(sample_restaurants_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read all reviews\n",
    "all_reviews = pd.read_csv(reviews_file).drop(labels='Unnamed: 0', axis=1).drop(labels='city', axis=1).drop(labels='state', axis=1).drop(labels='categories', axis=1).drop(labels='user_id', axis=1).drop(labels='date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Retain reviews of selected Businesses\n",
    "all_reviews = all_reviews[all_reviews.business_id.isin(all_restaurants.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Top 5 Reviews\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docs(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove new line and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String List cleaning, removes spaces, new lines\n",
    "def clean_string(data):\n",
    "    start = time.time()\n",
    "    data = remove_urls(data)\n",
    "    print(f\"URLs removed         - took {time.time() - start:9.4f} secs\")\n",
    "    \n",
    "    start = time.time()\n",
    "    data = remove_newlines(data)\n",
    "    print(f\"Removed line breaks  - took {time.time() - start:9.4f} secs\")\n",
    "    \n",
    "    start = time.time()\n",
    "    data = remove_spaces(data)\n",
    "    print(f\"Removed extra spaces - took {time.time() - start:9.4f} secs\")\n",
    "    \n",
    "    start = time.time()\n",
    "    data = remove_short_nots(data)\n",
    "    print(f\"Removed short Nots   - took {time.time() - start:9.4f} secs\")\n",
    "    \n",
    "    start = time.time()\n",
    "    data = split_on_space(data)\n",
    "    #data = list(tokenize_docs(data))\n",
    "    print(f\"Tokenized            - took {time.time() - start:9.4f} secs\")\n",
    "    \n",
    "    start = time.time()\n",
    "    data = remove_stop_words(data)\n",
    "    print(f\"Stopwords removed    - took {time.time() - start:9.4f} secs\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove stopwords from restautant names\n",
    "we need to remove restaurant names from reviews, otherwise these may potentially become topics (most frequent *nouns*). But restaurant names can have other words, such as chinese, grill etc. which should not be removed from reviews\n",
    "In below function, we cleanse restautant name so that only valid parts should be removed. This consistes of proper nouns whaich are not in stopwords for reataurant names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    name_toks = []\n",
    "    \n",
    "    # Nlp doc from Name\n",
    "    name_doc = nlp2(name)\n",
    "    for token in name_doc:\n",
    "        \n",
    "        # Retain Proper nouns in Name\n",
    "        if token.pos_ == 'PROPN' or token.like_num:\n",
    "        \n",
    "            # Lose stop words in Name\n",
    "            if token.text.lower() not in stopnames:\n",
    "            \n",
    "                # All Restaurant name tokens to be remoed from reviews of this reataurant\n",
    "                name_toks.append(token.text.lower())\n",
    "    \n",
    "    #for noun_phrase in list(name_doc.noun_chunks):\n",
    "        #if(len(str(noun_phrase).split())<2):\n",
    "            #noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_)\n",
    "    \n",
    "    \n",
    "    for chunk in name_doc.ents:\n",
    "        name_toks.append(chunk.text.lower())\n",
    "    \n",
    "    return name_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (all_reviews['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = clean_string(data)\n",
    "\n",
    "#  Stats\n",
    "## URLs removed         - took   120.258 secs\n",
    "## Removed line breaks  - took   42.2213 secs\n",
    "## Removed extra spaces - took   164.281 secs\n",
    "## Removed short Nots   - took   8.88918 secs\n",
    "## Tokenized            - took   67.6205 secs\n",
    "## Stopwords removed    - took   174.829 secs\n",
    "## Wall time: 9min 58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bigram  = gensim.models.Phrases(data, min_count=4, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data],min_count=3, threshold=100)  \n",
    "\n",
    "#  Stats\n",
    "## Wall time: 18min 9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bigram_mod  = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 5min 43s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bigrams  = [bigram_mod[doc] for doc in data]\n",
    "trigrams = [trigram_mod[bigram_mod[doc]] for doc in data]\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 21min 44s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#with open (\"processed_data/vocab.csv\",\"w\",encoding='utf-8')as vocab:\n",
    "#    vocab.write('\\n'.join(list(sorted(set(reduce(operator.concat, trigrams))))))\n",
    "#    \n",
    "#  Stats\n",
    "## Several Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_reviews['topic_text'] = [\" \".join(trigram).replace(\" .\",\".\\n\") for trigram in trigrams]\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 37.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoint\n",
    "#all_reviews.to_csv('processed_data/trigram_reviews.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_reviews     = pd.read_csv('processed_data/trigram_reviews.csv').drop(labels='Unnamed: 0', axis=1)\n",
    "#all_restaurants = pd.read_csv('processed_data/restaurants.csv').drop(labels='Unnamed: 0', axis=1).head(sample_restaurants_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove stopwords restautant reviews\n",
    "In below function, we cleanse restautant reviews for **Topic Modelling**. We revove all stop words, keep only nouns, verbs, adjectives and advesbs, and remove restautant references in reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc(doc):\n",
    "    \n",
    "    # Remove punctuation, symbols (#) and stopwords\n",
    "    topic_allowed_postags=['PROPN', 'NOUN', 'VERB']\n",
    "    sent_allowed_postags=['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV', 'DET', 'ADP']\n",
    "    \n",
    "    topics = []\n",
    "    sentis = []\n",
    "    \n",
    "    skip = False\n",
    "    for sent in doc.sents:\n",
    "        sent_words = []\n",
    "        for i,token in enumerate(sent):\n",
    "            if skip:\n",
    "                skip = False\n",
    "            else:\n",
    "                lemma = token.lemma_.strip().replace('_',' ')\n",
    "                word  = token.text.replace('_',' ')\n",
    "                pos   = token.pos_\n",
    "                if pos in topic_allowed_postags:\n",
    "                    topics.append(lemma.replace(\" \",\"_\"))\n",
    "                if pos in sent_allowed_postags:\n",
    "                    if i+1<len(sent) and pos in ['ADJ', 'ADV'] and sent[i+1].pos_ in ['NOUN', 'VERB']:\n",
    "                        sent_words.append(lemma+\"_\"+sent[i+1].lemma_)\n",
    "                        skip = True\n",
    "                    elif i+1<len(sent) and lemma in negations and sent[i+1].pos_ in ['ADJ', 'ADV'] and sent[i+1].lemma_ not in negations:\n",
    "                        sent_words.append(lemma+\"_\"+sent[i+1].lemma_)\n",
    "                        skip = True\n",
    "                    elif len(lemma.replace(\".\",\"\"))>1:\n",
    "                        sent_words.append(lemma.replace(\" \",\"_\"))\n",
    "        if len(sent_words)>0:\n",
    "            sent_words.append('.')\n",
    "        sentis.append(\" \".join(sent_words).replace(\" .\",\".\"))\n",
    "            \n",
    "    topic_text = str(\" \".join(topics).replace(\"\\n\",\" \"))\n",
    "    \n",
    "    sentiment_text = str(\" \".join((value for value in sentis if value != '.'))).replace(\"\\n\",\" \")\n",
    "    \n",
    "    return [topic_text, sentiment_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc, name_toks):\n",
    "    \n",
    "    sents  = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_words = []\n",
    "        for i,token in enumerate(sent) :\n",
    "            if token.lemma_ in list_diff([token.lemma_.lower() for token in sent],stopwords+name_toks) and token.lemma_ != \"PUNCT\":\n",
    "                sent_words.append(str(token.lemma_))\n",
    "        if len(sent_words)>0:\n",
    "            sent_words.append('.')\n",
    "        sents.append(\" \".join(sent_words))\n",
    "    \n",
    "    new_doc = str(\" \".join(sents).replace(\"  \",\" \").replace(\" .\",\"\").replace(\" .\",\".\").replace(\" .\",\".\").replace(\"..\",\".\"))\n",
    "    \n",
    "    return split_doc(nlp2(new_doc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total = len(all_restaurants)\n",
    "cleansed_text = []\n",
    "start = time.time()\n",
    "for index, restaurant in all_restaurants.iterrows():\n",
    "    #print(f'Cleaning reviews for restaurant: \"{restaurant[\"name\"]:<{40}}\" [{index+1:>{5}}/{total:>{5}}]')\n",
    "    if index % 500 == 0:\n",
    "        end = time.time()\n",
    "        print(f'Cleaning reviews [{index+1:>{5}}/{total:>{5}} ] - {str(end-start):>{9.6}} secs')\n",
    "        #with open (\"processed_data/data/cleansed_text_\"+str(index)+\".csv\",\"w\",encoding='utf-8')as f:\n",
    "        #   f.write('\\n'.join([sublist[0].replace('\\n',' ')+\",\"+sublist[1].replace('\\n',' ')for sublist in cleansed_text]))\n",
    "        #cleansed_text = []\n",
    "        start = time.time()\n",
    "    \n",
    "    # Convert to list\n",
    "    \n",
    "    data = all_reviews.query(' business_id == \"'+restaurant['business_id']+'\" ')['topic_text']\n",
    "    data = [u''+str(txt) for txt in data]\n",
    "    \n",
    "    # iterate list, clean sentences\n",
    "    for parsed_review in nlp2.pipe(iter(data), batch_size=5000, n_threads=20):\n",
    "        #[noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_) for noun_phrase in parsed_review.noun_chunks if len(str(noun_phrase).split())>1 and len(str(noun_phrase).split())<4]\n",
    "        cleansed_text.append(clean_doc(parsed_review,clean_name(restaurant[\"name\"])))\n",
    "        #pprint(parsed_review)\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "#Cleaning reviews [    1/17680 ] -    0.0060 secs\n",
    "#Cleaning reviews [  501/17680 ] -    1363.8 secs\n",
    "#Cleaning reviews [ 1001/17680 ] -    1490.7 secs\n",
    "#Cleaning reviews [ 1501/17680 ] -    1380.7 secs\n",
    "#Cleaning reviews [ 2001/17680 ] -    1484.2 secs\n",
    "#Cleaning reviews [ 2501/17680 ] -    1319.1 secs\n",
    "#Cleaning reviews [ 3001/17680 ] -    1504.2 secs\n",
    "#Cleaning reviews [ 3501/17680 ] -    1454.6 secs\n",
    "#Cleaning reviews [ 4001/17680 ] -    1410.8 secs\n",
    "#Cleaning reviews [ 4501/17680 ] -    1407.9 secs\n",
    "#Cleaning reviews [ 5001/17680 ] -    1453.4 secs\n",
    "#Cleaning reviews [ 5501/17680 ] -    1414.1 secs\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")\n",
    "\n",
    "all_reviews['topic_text']     = [el[0] for el in cleansed_text]\n",
    "all_reviews['sentiment_text'] = [el[1] for el in cleansed_text]\n",
    "\n",
    "all_reviews.to_csv('processed_data/'+state_filter+'_cleaned_reviews.csv',encoding='utf-8')\n",
    "all_restaurants.to_csv('processed_data/'+state_filter+'_cleaned_restaurants.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(set(reduce(operator.concat, all_reviews['topic_text']).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open ('processed_data/'+state_filter+'_Topicp_Vocab.csv',\"w\",encoding='utf-8')as vocab:\n",
    "    vocab.write('\\n'.join(list(sorted(set(reduce(operator.concat, all_reviews['topic_text']).split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_restaurants = pd.read_csv(restaurant_file,encoding='utf-8').drop(labels='Unnamed: 0', axis=1).head(sample_restaurants_to_load)\n",
    "all_reviews = pd.read_csv(reviews_file,encoding='utf-8').drop(labels='Unnamed: 0', axis=1).drop(labels='categories', axis=1).drop(labels='user_id', axis=1).drop(labels='date', axis=1)\n",
    "Restaurant_reviews = pd.merge(Restaurant_business,All_reviews, on='business_id').rename(columns={'stars_x':'business_stars', 'stars_y':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.head(5)['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_dict = {}\n",
    "for i, row in all_reviews.iterrows():\n",
    "    if i % 100000 == 0:\n",
    "        print(f'{i} / {len(all_reviews)}')\n",
    "    state = row['state']\n",
    "    city  = str(row.city)\n",
    "    name  = str(row['name'])\n",
    "    review_id = str(row['review_id'])\n",
    "    restaurants_dict[str(state)]= restaurants_dict.setdefault(str(state), {})\n",
    "    restaurants_dict[str(state)][str(city)] = restaurants_dict[str(state)].setdefault(str(city), {})\n",
    "    restaurants_dict[str(state)][str(city)][str(name)] = restaurants_dict[str(state)][str(city)].setdefault(str(name), [])\n",
    "    if len(restaurants_dict[str(state)][str(city)][str(name)])<=15:\n",
    "        restaurants_dict[str(state)][str(city)][str(name)].append(str(review_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('pickles/select_list_dic.pk', 'wb') as f:\n",
    "    pickle.dump(restaurants_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(restaurants_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_dict['AB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read the Pre-Processed data\n",
    "restaurant_file = pd.read_csv('processed_data/cleaned_restaurants.csv')\n",
    "restaurant_file = restaurant_file.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "review_file     = pd.read_csv('processed_data/cleaned_reviews.csv')\n",
    "review_file     = review_file.drop(labels=\"Unnamed: 0\", axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert in Unicode\n",
    "# seperate topic relavant data from sentiment relavent data\n",
    "review_file['topic_text'] = [u''+str(txt).replace('.','').replace('\\\\n','') for txt in review_file.topic_text]\n",
    "review_file['sentiment_text'] = [u''+str(txt).replace('.','').replace('\\\\n','') for txt in review_file.sentiment_text]\n",
    "review_file['text'] = [u''+str(txt) for txt in review_file.text]\n",
    "review_file['review_length'] = review_file.text.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's check the distribution of the reviews by length of comments\n",
    "ax = sns.FacetGrid(data=review_file, col='review_stars', xlim=(0, 1000)).map(plt.hist, 'review_length', bins=50)\n",
    "ax.axes[0][0].set(ylabel='Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's interesting that five star reivews are more verbose\n",
    "#print(review_file.loc[review_file['review_stars']==1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews_sent  = review_file.sentiment_text[review_file.review_stars>3].values\n",
    "neg_reviews_sent  = review_file.sentiment_text[review_file.review_stars<3].values\n",
    "pos_reviews_topic = review_file.topic_text[review_file.review_stars>3].values\n",
    "neg_reviews_topic = review_file.topic_text[review_file.review_stars<3].values\n",
    "\n",
    "all_reviews_topic = review_file.topic_text.values\n",
    "all_reviews_sent  = review_file.sentiment_text.values\n",
    "\n",
    "print('Postive  Reviews: {:,}'.format(len(pos_reviews_topic)))\n",
    "print('Negative Reviews: {:,}'.format(len(neg_reviews_topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews_topic = pos_reviews_sent\n",
    "neg_reviews_topic = neg_reviews_sent\n",
    "all_reviews_topic = all_reviews_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take our additional stopwords\n",
    "#from sklearn.feature_extraction import text\n",
    "#extra_words = ['like','did','said','ok', 've', 'got']\n",
    "#stop_words = text.ENGLISH_STOP_WORDS.union(extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')] \n",
    "neg_words  = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')]\n",
    "stop_words = stop_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max features for data vertorication\n",
    "max_features=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "def tokenizer(x):\n",
    "    return ( w for w in str(x).split() if len(w) >3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#TF-IDF\n",
    "# Create a vectorizer object to generate term document counts\n",
    "tfidf_pos = TfidfVectorizer(stop_words=stop_words, min_df=10, max_df=0.5,\n",
    "                        tokenizer = tokenizer,ngram_range=(1,1), max_features=max_features)\n",
    "                        #ngram_range=(1,1), token_pattern='[a-z][a-z]+', max_features=max_features)\n",
    "\n",
    "tfidf_neg = TfidfVectorizer(stop_words=stop_words, min_df=10, max_df=0.5, \n",
    "                        tokenizer = tokenizer,ngram_range=(1,1), max_features=max_features)\n",
    "                        #ngram_range=(1,1), token_pattern='[a-z][a-z]+', max_features=max_features)\n",
    "\n",
    "tfidf_all = TfidfVectorizer(stop_words=stop_words, min_df=10, max_df=0.5, \n",
    "                        tokenizer = tokenizer,ngram_range=(1,1), max_features=max_features)\n",
    "                        #ngram_range=(1,1), token_pattern='[a-z][a-z]+', max_features=max_features)\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 2min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Vectorize Data\n",
    "neg_vectors  = tfidf_neg.fit_transform(neg_reviews_topic)\n",
    "pos_vectors  = tfidf_pos.fit_transform(pos_reviews_topic)\n",
    "all_vectors  = tfidf_all.fit_transform(all_reviews_topic)\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 1min 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/topic_term_vector_all.pk','wb') as f:\n",
    "    pickle.dump(tfidf_all, f)\n",
    "with open('pickles/topic_term_vector_pos.pk','wb') as f:\n",
    "    pickle.dump(tfidf_pos, f)\n",
    "with open('pickles/topic_term_vector_neg.pk','wb') as f:\n",
    "    pickle.dump(tfidf_neg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_feature_names = np.array(tfidf_neg.get_feature_names())\n",
    "pos_feature_names = np.array(tfidf_pos.get_feature_names())\n",
    "all_feature_names = np.array(tfidf_all.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/neg_topic_vectors_'+str(max_features)+'.pk', 'wb') as fin:\n",
    "#    pickle.dump(neg_vectors, fin)\n",
    "with open('pickles/neg_topic_features_'+str(max_features)+'.pk', 'wb') as fin:\n",
    "    pickle.dump(np.array(tfidf_neg.get_feature_names()), fin)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/pos_topic_vectors_'+str(max_features)+'.pk', 'wb') as fin:\n",
    "#    pickle.dump(pos_vectors, fin)\n",
    "with open('pickles/pos_topic_features_'+str(max_features)+'.pk', 'wb') as fin:\n",
    "    pickle.dump(np.array(tfidf_pos.get_feature_names()), fin)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/all_topic_vectors_'+str(max_features)+'.pk', 'wb') as fin:\n",
    "#    pickle.dump(all_vectors, fin)\n",
    "with open('pickles/all_topic_features_'+str(max_features)+'.pk', 'wb') as fin:\n",
    "    pickle.dump(np.array(tfidf_all.get_feature_names()), fin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/neg_topic_vectors_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "#    neg_vectors = pickle.load(fin)\n",
    "with open('pickles/neg_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    neg_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/pos_topic_vectors_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "#    pos_vectors = pickle.load(fin)\n",
    "with open('pickles/pos_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    pos_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/all_topic_vectors_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "#    all_vectors = pickle.load(fin)\n",
    "with open('pickles/all_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    all_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LSA\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# Need to transpose it for gensim which wants terms by docs instead of docs by terms\n",
    "tfidf_pos_corpus = matutils.Sparse2Corpus(pos_vectors.transpose())\n",
    "tfidf_neg_corpus = matutils.Sparse2Corpus(neg_vectors.transpose())\n",
    "tfidf_all_corpus = matutils.Sparse2Corpus(all_vectors.transpose())\n",
    "\n",
    "# Row indices\n",
    "id2word_pos = dict((v,k) for k,v in tfidf_pos.vocabulary_.items())\n",
    "id2word_neg = dict((v,k) for k,v in tfidf_neg.vocabulary_.items())\n",
    "id2word_all = dict((v,k) for k,v in tfidf_all.vocabulary_.items())\n",
    "\n",
    "# This is a hack for Python 3!\n",
    "id2word_pos = corpora.Dictionary.from_corpus(tfidf_pos_corpus, id2word=id2word_pos)\n",
    "id2word_neg = corpora.Dictionary.from_corpus(tfidf_neg_corpus, id2word=id2word_neg)\n",
    "id2word_all = corpora.Dictionary.from_corpus(tfidf_all_corpus, id2word=id2word_all)\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 1min 21s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create LSA model\n",
    "lsi_pos = models.LsiModel(tfidf_pos_corpus, id2word=id2word_pos, num_topics=NUM_TOPICS)\n",
    "lsi_neg = models.LsiModel(tfidf_neg_corpus, id2word=id2word_neg, num_topics=NUM_TOPICS)\n",
    "lsi_all = models.LsiModel(tfidf_all_corpus, id2word=id2word_all, num_topics=NUM_TOPICS)\n",
    "\n",
    "#  Stats\n",
    "## Wall time: 13min 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# negative values in LSA topics are annoying\n",
    "# let's see if we can fix that with NMF\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# change num_topics\n",
    "num_topics = NUM_TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nmf_pos = NMF(n_components=num_topics)\n",
    "W_pos = nmf_pos.fit_transform(pos_vectors)\n",
    "H_pos = nmf_pos.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nmf_neg = NMF(n_components=num_topics)\n",
    "W_neg = nmf_neg.fit_transform(neg_vectors)\n",
    "H_neg = nmf_neg.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nmf_all = NMF(n_components=num_topics)\n",
    "W_all = nmf_all.fit_transform(all_vectors)\n",
    "H_all = nmf_all.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing: dump processed won in pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for future use\n",
    "with open('pickles/neg_topic_model.pk', 'wb') as fin:\n",
    "    pickle.dump(lsi_neg, fin)\n",
    "with open('pickles/pos_topic_model.pk', 'wb') as fin:\n",
    "    pickle.dump(lsi_pos, fin)\n",
    "with open('pickles/all_topic_model.pk', 'wb') as fin:\n",
    "    pickle.dump(lsi_all, fin)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for future use\n",
    "with open('pickles/neg_nmf_model.pk', 'wb') as fin:\n",
    "    pickle.dump(nmf_neg, fin)\n",
    "with open('pickles/pos_nmf_model.pk', 'wb') as fin:\n",
    "    pickle.dump(nmf_pos, fin)\n",
    "with open('pickles/all_nmf_model.pk', 'wb') as fin:\n",
    "    pickle.dump(nmf_all, fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load saved pickles and resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=2500\n",
    "num_topics  =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/all_topic_model.pk', 'rb') as fin:\n",
    "    lsi_all = pickle.load(fin)\n",
    "with open('pickles/pos_topic_model.pk', 'rb') as fin:\n",
    "    lsi_pos = pickle.load(fin)\n",
    "with open('pickles/neg_topic_model.pk', 'rb') as fin:\n",
    "    lsi_neg = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/neg_nmf_model.pk', 'rb') as fin:\n",
    "    nmf_neg = pickle.load(fin)\n",
    "with open('pickles/pos_nmf_model.pk', 'rb') as fin:\n",
    "    nmf_pos = pickle.load(fin)\n",
    "with open('pickles/all_nmf_model.pk', 'rb') as fin:\n",
    "    nmf_all = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/all_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    all_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/pos_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    pos_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/neg_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    neg_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/topic_term_vector_all.pk', 'rb') as fin:\n",
    "    term_vector = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_list(model, feature_names, num_topics, no_top_words):\n",
    "    lst = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx < num_topics:\n",
    "            #print(\"{:11}\".format(\"Topic %d:\" %(topic_idx)), end='')\n",
    "            #print(\", \".join(['{:04.3f}*'.format(topic[i])+feature_names[i] for i in topic.argsort()[:-no_top_words-1:-1]]))\n",
    "            lst.append('/'.join(sort_list([feature_names[i]+\" \" for i in topic.argsort()[:-no_top_words-1:-1]])))\n",
    "    return((list((set(lst)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_map(model, feature_names, num_topics, no_top_words):\n",
    "    dicty = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx < num_topics:\n",
    "            dicty[topic_idx]='/'.join(sort_list([feature_names[i]+\" \" for i in topic.argsort()[:-no_top_words-1:-1]]))\n",
    "    return dicty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_topics, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx < num_topics:\n",
    "            print(\"{:11}\".format(\"Topic %d:\" %(topic_idx)), end='')\n",
    "            print(\", \".join(['{:04.3f}*'.format(topic[i])+feature_names[i] \\\n",
    "                             for i in topic.argsort()[:-no_top_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_to_text(topic):\n",
    "    str = ''\n",
    "    for elem in list(topic)[1].split('+')[:6]:\n",
    "        str+= '/'.join(re.findall(r'\"([^\"]*)\"', elem))+' '\n",
    "    topics.append(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list(lst):\n",
    "    return list(sorted(set(reduce(operator.concat, lst).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "no_topics = num_topics\n",
    "no_top_words = 4\n",
    "\n",
    "topic_list.extend(get_topic_list(nmf_all, all_feature_names, no_topics, no_top_words))\n",
    "topic_list.extend(get_topic_list(nmf_pos, pos_feature_names, no_topics, no_top_words))\n",
    "topic_list.extend(get_topic_list(nmf_neg, neg_feature_names, no_topics, no_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/topic_map.csv','w+') as f:\n",
    "    f.write('\\n'.join(topic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_all, all_feature_names, no_topics, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_map(nmf_all, all_feature_names, no_topics, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/all_topics.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(key)+\";\"+str(value) for key, value in get_topic_map(nmf_all, all_feature_names, no_topics, no_top_words).items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/neg_topics.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(key)+\";\"+str(value) for key, value in get_topic_map(nmf_neg, neg_feature_names, no_topics, no_top_words).items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/pos_topics.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(key)+\";\"+str(value) for key, value in get_topic_map(nmf_pos, pos_feature_names, no_topics, no_top_words).items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('config/topics.csv', 'w+') as f:\n",
    "#    f.write('\\n'.join(topic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read the Pre-Processed data\n",
    "restaurant_file = pd.read_csv('processed_data/cleaned_restaurants.csv')\n",
    "restaurant_file = restaurant_file.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "review_file     = pd.read_csv('processed_data/cleaned_reviews.csv')\n",
    "review_file     = review_file.drop(labels=\"Unnamed: 0\", axis=1) \n",
    "\n",
    "review_file['topic_text'] = [u''+str(txt).replace('.','').replace('\\\\n','') for txt in review_file.topic_text]\n",
    "review_file['sentiment_text'] = [u''+str(txt).replace('.','').replace('\\\\n','') for txt in review_file.sentiment_text]\n",
    "review_file['text'] = [u''+str(txt) for txt in review_file.text]\n",
    "review_file['review_length'] = review_file.text.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    return ( w for w in str(x).split() if len(w) >3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=5000\n",
    "\n",
    "with open('pickles/neg_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    neg_feature_names = pickle.load(fin)\n",
    "with open('pickles/pos_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    pos_feature_names = pickle.load(fin)\n",
    "with open('pickles/all_topic_features_'+str(max_features)+'.pk', 'rb') as fin:\n",
    "    all_feature_names = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desc_map = {}\n",
    "with open('config/topic_map.txt', 'r') as f:\n",
    "    topic_desc_map = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "\n",
    "neg_topic_map = {}\n",
    "pos_topic_map = {}\n",
    "all_topic_map = {}\n",
    "\n",
    "with open('config/pos_topics.txt', 'r') as f:\n",
    "    pos_topic_map = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "with open('config/neg_topics.txt', 'r') as f:\n",
    "    neg_topic_map = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "with open('config/all_topics.txt', 'r') as f:\n",
    "    all_topic_map = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    \n",
    "with open('pickles/topic_term_vector_all.pk', 'rb') as fin:\n",
    "    term_vector = pickle.load(fin)\n",
    "with open('pickles/topic_term_vector_neg.pk', 'rb') as fin:\n",
    "    neg_vectors = pickle.load(fin)\n",
    "with open('pickles/topic_term_vector_pos.pk', 'rb') as fin:\n",
    "    pos_vectors = pickle.load(fin)\n",
    "with open('pickles/topic_term_vector_all.pk', 'rb') as fin:\n",
    "    all_vectors = pickle.load(fin)\n",
    "    \n",
    "with open('pickles/all_nmf_model.pk', 'rb') as fin:\n",
    "    nmf_all = pickle.load(fin)\n",
    "with open('pickles/pos_nmf_model.pk', 'rb') as fin:\n",
    "    nmf_pos = pickle.load(fin)\n",
    "with open('pickles/neg_nmf_model.pk', 'rb') as fin:\n",
    "    nmf_neg = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics = review_file[['business_id','name','review_id','review_stars','text']]\n",
    "review_topics = review_topics.reindex(review_topics.columns.tolist() + ['topic','pos_topic','neg_topic'], axis=1)\n",
    "review_topics = review_topics.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text = review_file[\"sentiment_text\"].tolist()\n",
    "text = [u''+str(txt) for txt in text]\n",
    "star = review_file[\"review_stars\"].tolist()\n",
    "vec = term_vector.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "avec = all_vectors.transform(text)\n",
    "pvec = pos_vectors.transform(text)\n",
    "nvec = neg_vectors.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "txt_topic = list(itemgetter(*itemgetter(*([str(np.argmax(res)) for res in nmf_all.transform(avec)]))(all_topic_map))(topic_desc_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pos_topic = list(itemgetter(*itemgetter(*([str(np.argmax(res)) for res in nmf_pos.transform(pvec)]))(pos_topic_map))(topic_desc_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neg_topic = list(itemgetter(*itemgetter(*([str(np.argmax(res)) for res in nmf_neg.transform(nvec)]))(neg_topic_map))(topic_desc_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(review_file)):\n",
    "    if star[index]<3:\n",
    "        pos_topic[index]=''\n",
    "    if star[index]>3:\n",
    "        neg_topic[index]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics['topic']     = txt_topic\n",
    "review_topics['pos_topic'] = pos_topic\n",
    "review_topics['neg_topic'] = neg_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics.text.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics.neg_topic.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics.to_csv('processed_data/review_topics.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_texts(str_text, nmf_all, nmf_pos, nmf_neg, all_topic_map, pos_topic_map, neg_topic_map, topic_desc_map, all_vectors, pos_vectors, neg_vectors):\n",
    "    text = [(u''+str(str_text))]\n",
    "    \n",
    "    avec = all_vectors.transform(text)\n",
    "    pvec = pos_vectors.transform(text)\n",
    "    nvec = neg_vectors.transform(text)\n",
    "    \n",
    "    txt_topic_text = (topic_desc_map[all_topic_map[str(np.argmax(nmf_all.transform(avec)))]])\n",
    "    pos_topic_text = (topic_desc_map[pos_topic_map[str(np.argmax(nmf_pos.transform(pvec)))]])\n",
    "    neg_topic_text = (topic_desc_map[neg_topic_map[str(np.argmax(nmf_neg.transform(nvec)))]])\n",
    "    \n",
    "    return [txt_topic_text,pos_topic_text,neg_topic_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_vector():\n",
    "    with open('pickles/topic_term_vector_all.pk', 'rb') as fin:\n",
    "        return pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_all():\n",
    "    with open('pickles/all_nmf_model.pk', 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "\n",
    "def get_nmf_pos():\n",
    "    with open('pickles/pos_nmf_model.pk', 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "\n",
    "def get_nmf_neg():\n",
    "    with open('pickles/neg_nmf_model.pk', 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "    \n",
    "def get_pos_topics_map():\n",
    "    with open('config/pos_topics.txt', 'r') as f:\n",
    "        ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret\n",
    "\n",
    "def get_neg_topics_map():\n",
    "    with open('config/neg_topics.txt', 'r') as f:\n",
    "        ret =  {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret\n",
    "    \n",
    "def get_all_topics_map():\n",
    "    with open('config/all_topics.txt', 'r') as f:\n",
    "         ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret\n",
    "\n",
    "def get_topic_desc_map():\n",
    "    with open('config/topic_map.txt', 'r') as f:\n",
    "        ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret\n",
    "\n",
    "def get_all_term_vec():\n",
    "    with open('pickles/topic_term_vector_all.pk','rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_pos_term_vec():\n",
    "    with open('pickles/topic_term_vector_pos.pk','rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_neg_term_vec():\n",
    "    with open('pickles/topic_term_vector_neg.pk','rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_all     = get_nmf_all()\n",
    "nmf_pos     = get_nmf_pos()\n",
    "nmf_neg     = get_nmf_neg()\n",
    "all_topic_map = get_all_topics_map()\n",
    "pos_topic_map = get_pos_topics_map()\n",
    "neg_topic_map = get_neg_topics_map()\n",
    "topic_desc_map = get_topic_desc_map()    \n",
    "pos_vec     = get_pos_term_vec()\n",
    "neg_vec     = get_neg_term_vec()\n",
    "all_vec     = get_all_term_vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file.sentiment_text.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_text = \"'horrible-service boyfriend because pass couple time try bok choy-chicken fry rice lemon chicken food chicken hard-look old like refried complain server not-acknowledge complaint leave tell server tell leave offering provide new finally-woman server offer new-agree old minute rice bok-choy time bring not-hungry consider not-want chicken rice cold exception sweet old-lady serve rest server rude server table check bill much-tip couple leave hear mention tip bill throw angry disgusting-will not-return\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics_texts(str_text, nmf_all, nmf_pos, nmf_neg, all_topic_map, pos_topic_map, neg_topic_map, topic_desc_map, all_vec, pos_vec, neg_vec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_topic_data():\n",
    "    return pd.read_csv('processed_data/cleaned_reviews.csv', usecols = [\"business_id\", \"name\", \"review_id\",\"text\",\"topic_text\",\"sentiment_text\",\"review_stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_topic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')] \n",
    "neg_words  = [line.rstrip('\\n') for line in open('config/negations.txt', 'r', encoding='utf-8')]\n",
    "stopwords = stop_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(list1,list2):\n",
    "    return list(itertools.filterfalse(set(list2).__contains__, list1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_phrases(str_list):\n",
    "    new_list = []\n",
    "    for tok in str_list:\n",
    "        if '-' in tok:\n",
    "            new_list += tok.split('-')\n",
    "        new_list.append(tok)\n",
    "    return list_diff(new_list,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_set = [split_phrases((u''+str(txt)).replace('.','').split()) for txt in data.topic_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_set = [split_phrases((u''+str(txt)).replace('.','').split()) for txt in data.sentiment_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#build vocabulary and train model\n",
    "topic_model = gensim.models.Word2Vec(\n",
    "        topic_set,\n",
    "        size=100,\n",
    "        window=10,\n",
    "        min_count=100,\n",
    "        workers=100,\n",
    "        iter=10)\n",
    "\n",
    "#  Stats\n",
    "## NC (0250 MB): Wall time:   14.8 s /   22 s\n",
    "## ALL         : 4min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#build vocabulary and train model\n",
    "senti_model = gensim.models.Word2Vec(\n",
    "        senti_set,\n",
    "        size=100,\n",
    "        window=15,\n",
    "        min_count=100,\n",
    "        workers=100,\n",
    "        iter=20)\n",
    "\n",
    "#  Stats\n",
    "## NC (0250 MB): Wall time:   14.8 s /   22 s\n",
    "## ALL         : Wall time: 17min 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('pickles/aspect_topic_model.pk', 'wb') as fin:\n",
    "#    pickle.dump(topic_model, fin)\n",
    "#with open('pickles/aspect_senti_model.pk', 'wb') as fin:\n",
    "#    pickle.dump(senti_model, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model = None\n",
    "topic_model = None\n",
    "with open('pickles/aspect_topic_model.pk', 'rb') as fin:\n",
    "    topic_model = pickle.load(fin)\n",
    "with open('pickles/aspect_senti_model.pk', 'rb') as fin:\n",
    "    senti_model = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.wv.most_similar(positive='cusine',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.most_similar(positive='far',topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.similarity(w1='location',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.similarity(w1='staff',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.similarity(w1='atmosphere',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.similarity(w1='price',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.similarity(w1='service',w2='far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aspect_Terms = ['food','service','staff','location','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_topics_map():\n",
    "    with open('config/all_topics.txt', 'r') as f:\n",
    "         ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = get_all_topics_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for x in topic_map.values():\n",
    "    for y in x.split('/'):\n",
    "        vocab.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_map():\n",
    "    with open('config/aspect_map.txt', 'r') as f:\n",
    "        ret = {line.split(';')[0]: line.split(';')[1].replace('\\n','') for line in f.readlines()}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_map = get_aspect_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_aspect(sent,aspect_map):\n",
    "    dicty = {}\n",
    "    \n",
    "    for word in nlp(sent.lower()):\n",
    "        if word.lemma_ not in stopwords:\n",
    "            score = 0\n",
    "            aspec = None\n",
    "            for asp in aspect_map.keys():\n",
    "                try:\n",
    "                    sco = senti_model.wv.similarity(w1=asp,w2=word.lemma_)\n",
    "                    if sco > score:\n",
    "                        score=sco\n",
    "                        aspec=asp\n",
    "                        dicty[aspec]= dicty.setdefault(asp, 0) + score    \n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "    if len(dicty)==0:\n",
    "        return None\n",
    "    \n",
    "    ret = max(dicty.items(), key=lambda k: k[1])\n",
    "    if ret[1]<0.3:\n",
    "        return None\n",
    "    \n",
    "    return aspect_map[ret[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lst = []\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "total = len(data)\n",
    "for index,txt in enumerate(data.text):\n",
    "    mydict = {}\n",
    "    for w in Aspect_Terms:\n",
    "        mydict[w] = mydict.setdefault(w, '')  \n",
    "    for x in txt.replace('\\\\n','.').replace('\\n','.').split('.'):\n",
    "        if(len(x)>0):\n",
    "            y = assign_aspect(x,aspect_map)\n",
    "            mydict[y] = mydict.setdefault(y, '') + x +'.\\n' \n",
    "    mydict.pop(None, None)\n",
    "    lst.append(mydict)\n",
    "    \n",
    "    if index%1000 == 0 and index > 0:\n",
    "        print(f\".\", end='')\n",
    "            \n",
    "    if index%10000 == 0 and index > 0:\n",
    "        end = time.time()\n",
    "        print(f' Clustered [{index+1:>{5}}/{total:>{5}} ] - {str(end-start):>{9.6}} secs')\n",
    "        start = time.time()\n",
    "        with open('pickles/IL/'+str(index)+'.pk', 'wb') as fin:\n",
    "            pickle.dump(lst, fin)\n",
    "        lst = []\n",
    "        \n",
    "print(f'\\n Clustered [{total:>{5}} ] - {str(end-start):>{9.6}} secs')\n",
    "\n",
    "with open('pickles/IL/3530000.pk', 'wb') as fin:\n",
    "    pickle.dump(lst, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for index in range(len(data)):\n",
    "    if index%10000 == 0 and index > 0:\n",
    "        with open('pickles/IL/'+str(index)+'.pk', 'rb') as fin:\n",
    "            lst += pickle.load(fin)\n",
    "        print(index)\n",
    "with open('pickles/IL/3530000.pk', 'rb') as fin:\n",
    "    lst += pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_df = pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df = data[['review_id','text','review_stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asp in Aspect_Terms:\n",
    "    aspects_df[asp] = aspect_df[asp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df.to_csv('processed_data/aspect_review.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_aspect('A bar with bar food. There were four of us.',aspect_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model.wv.similarity(w1='bar',w2='food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dir = os.path.join('pickles/sentiment_analysis')\n",
    "cleaned_reviews_file = os.path.join(pickle_dir, 'cleaned_reviews_df.pkl')\n",
    "df_classes_file = os.path.join(pickle_dir, 'df_classes.pkl')\n",
    "vocab_file = os.path.join(pickle_dir, 'cleaned_reviews_vocab.pkl')\n",
    "transformed_sentiment_file = os.path.join(pickle_dir, 'cleaned_reviews_x_sentiment.pkl')\n",
    "classifier_file = os.path.join(pickle_dir, 'mnb_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install missing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} s3fs seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_bucket(bucket, data_key):\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "    chunksize = 1000000\n",
    "    chunk_list = []\n",
    "    df_chunk = pd.read_csv(data_location, chunksize=chunksize)\n",
    "    for chunk in df_chunk:\n",
    "        chunk_list.append(chunk)\n",
    "\n",
    "    df = pd.concat(chunk_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if 1 == 1:\n",
    "    \n",
    "    bucket = 'cs410-yelp'\n",
    "    data_key = 'processed_data/cleaned_reviews.csv'\n",
    "\n",
    "    df = read_s3_bucket(bucket, data_key)\n",
    "    df = df.drop(labels='Unnamed: 0', axis=1)\n",
    "    df['review_stars']   = df['review_stars'].astype(int)\n",
    "    df['sentiment_text'] = df['sentiment_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stval = df.groupby('review_stars').mean()\n",
    "stval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [line.rstrip('\\n') for line in open('config/stopwords.txt', 'r', encoding='utf-8')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = frozenset(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CLASSIFICATION\n",
    "df_classes = df[(df['review_stars'] == 1) | (df['review_stars'] == 3) | (df['review_stars'] == 5)]\n",
    "df_classes = df_classes[(df_classes['useful'] == 1)]\n",
    "df_classes.head()\n",
    "print(df_classes.shape)\n",
    "\n",
    "# Seperate the data set into X and Y for prediction\n",
    "x = df_classes['sentiment_text']\n",
    "y = df_classes['review_stars']\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "r0 = x[3]\n",
    "print(r0)\n",
    "vocab = CountVectorizer(analyzer=text_process,stop_words=stopwords).fit(x)\n",
    "print(len(vocab.vocabulary_))\n",
    "vocab0 = vocab.transform([r0])\n",
    "print(vocab0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization of the whole review set and and checking the sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = vocab.transform(x)\n",
    "#Shape of the matrix:\n",
    "print(\"Shape of the sparse matrix: {}\".format(x.shape))\n",
    "#Non-zero occurences:\n",
    "print(\"Non-Zero occurences: {}\".format(x.nnz))\n",
    "\n",
    "# DENSITY OF THE MATRIX\n",
    "density = (x.nnz / (x.shape[0] * x.shape[1])) * 100\n",
    "print(\"Density of the matrix: {}\".format(density))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data set into training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_pred, classifier_name):\n",
    "    print(\"Confusion Matrix for {}:\".format(classifier_name))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['1-Star', '3-Star', '5-Star']))\n",
    "    print(\"\\nScore: {}\".format(round(accuracy_score(y_true, y_pred)*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train, y_train)\n",
    "predmnb = mnb.predict(x_test)\n",
    "print_results(y_test, predmnb, \"Multinomial Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 11\n",
    "pr = df['sentiment_text'][item]\n",
    "print(pr)\n",
    "print(\"\\nActual Rating: {}\".format(df['review_stars'][item]))\n",
    "pr_t = vocab.transform([pr])\n",
    "print(\"Predicted Rating: {}\".format(mnb.predict(pr_t)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(cleaned_reviews_file, 'wb') as file:\n",
    "    pickle.dump(df, file)\n",
    "\n",
    "with open(df_classes_file, 'wb') as file:\n",
    "    pickle.dump(df_classes, file)\n",
    "\n",
    "with open(vocab_file, 'wb') as file:\n",
    "    pickle.dump(vocab, file)\n",
    "\n",
    "with open(transformed_sentiment_file, 'wb') as file:\n",
    "    pickle.dump(x, file)\n",
    "\n",
    "with open(classifier_file, 'wb') as file:\n",
    "    pickle.dump(mnb, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
